.. _model-hub-transformers:

########################
 Model Hub Transformers
########################

**********
 Overview
**********

`The Huggingface transformers library
<https://github.com/huggingface/transformers>`_ is the de-facto library for
natural language processing models. It provides pretrained weights for leading
NLP models and allows users to easily use these pretrained models for the most
common NLP tasks like language modeling, text classification, and question
answering among others. 

**model-hub** makes it easy for users to train transformer models in Determined
while keeping the developer experience as close as possible to what it's like
working directly with the transformers library. Our library serves as an
alternative to HuggingFace's `Trainer Class
<https://huggingface.co/transformers/main_classes/trainer.html>`_.  Using our
library will provide access to some compelling benefits:

-  Easy :ref:`multi-node distributed training <multi-gpu-training>` with
   no code modifications. Determined automatically sets up the
   distributed backend for you.

-  Integrated experiment tracking, experiment visualization, artifact tracking,
   and :ref:`state-of-the-art hyperparameter search <hyperparameter-tuning>`.

-  :ref:`Automated cluster management, fault tolerance, and job
   rescheduling <benefits-of-determined>` so you don't have to worry
   about provisioning resources or babysit your experiments.

Given the benefits above, we think this library will be particularly
useful to you if any of the following apply:

-  You are an existing user of Determined that wants to get started
   quickly using **transformers** in Determined.

-  You are a **transformers** user that wants to easily run more advanced
   workflows with **transformers** like multi-node distributed training and
   advanced hyperparameter search.

-  You are a **transformers** user looking for a single platform to manage
   experiments, handle checkpoints with automated fault tolerance, and
   perform hyperparameter search/visualization.

**********
 Getting Started
**********

The easiest way to use Model Hub Transformers is to start with an existing
example Trial. Model Hub Transformers includes thoroughly tested
:ref:`implementations of all core transformers tasks
<model-hub-transformers-examples>`.

Model Hub Transformers Trials are infinitely customizable. Learn about how to
customize or build your own Trial by checking out the :ref:`Model Hub
Transformers Tutorial <model-hub-transformers-tutorial>`.

**********
 Limitations
**********

While  we strive to offer as many of the features supported in Huggingface
**transformers**, the following features are not currently supported:

-  Tensorflow version of transformers
-  Suport for deepspeed and fairscale
-  Running on TPUs

.. toctree::
   :maxdepth: 3
   :hidden:

   tutorial
   examples
   api
